{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_allocine_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZEx7LJe6rfPIRWH59KLjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nour-Mws/french_sentiment_analysis_on_tweets/blob/main/training_allocine_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGkIW3q1Wcpk"
      },
      "source": [
        "# Imports et fonctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfKyHrRZWkHx",
        "outputId": "2ab1349b-c66f-4f9c-b0a0-5ff99fdc5b82"
      },
      "source": [
        "!pip install transformers>=4.0\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 14.8MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFWqMfQyV1j-"
      },
      "source": [
        "import numpy as np\r\n",
        "import pickle\r\n",
        "from sklearn.base import BaseEstimator\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.base import TransformerMixin\r\n",
        "import tensorflow as tf\r\n",
        "import time\r\n",
        "from transformers import CamembertTokenizer\r\n",
        "from transformers import TFCamembertForSequenceClassification"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjpgI-U3WFS4"
      },
      "source": [
        "class CamembertPreprocessor(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, tokenizer, max_seq_length):\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_seq_length = max_seq_length\r\n",
        "\r\n",
        "    def fit(self, X=None):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def transform(self, X, y):\r\n",
        "        # 1. Tokenize\r\n",
        "        X_encoded = encode_reviews(self.tokenizer, X, self.max_seq_length)\r\n",
        "        # 2. Labels\r\n",
        "        y_array = np.array(y)\r\n",
        "        return X_encoded, y_array\r\n",
        "\r\n",
        "    def fit_transform(self, X, y):\r\n",
        "        return self.transform(X, y)\r\n",
        "\r\n",
        "\r\n",
        "class EarlyStoppingModel(BaseEstimator):\r\n",
        "    def __init__(\r\n",
        "        self, transformers_model, max_epoches, batch_size, validation_data):\r\n",
        "        self.model = transformers_model\r\n",
        "        self.max_epoches = max_epoches\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.validation_data = validation_data\r\n",
        "\r\n",
        "    def fit(self, X, y):\r\n",
        "        # Defines early stopper\r\n",
        "        early_stopper = tf.keras.callbacks.EarlyStopping(\r\n",
        "            monitor='val_loss', mode='auto', patience=2, # only 1 !\r\n",
        "            verbose=1, restore_best_weights=True\r\n",
        "        )\r\n",
        "\r\n",
        "        # Train model on data subset\r\n",
        "        self.model.fit(\r\n",
        "            X, y,\r\n",
        "            validation_data=self.validation_data,\r\n",
        "            epochs=self.max_epoches,\r\n",
        "            batch_size=self.batch_size,\r\n",
        "            callbacks=[early_stopper],\r\n",
        "            verbose=1\r\n",
        "        )\r\n",
        "        return self\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        scores = self.model.predict(X)\r\n",
        "        y_pred = np.argmax(scores, axis=1)\r\n",
        "        return y_pred\r\n",
        "\r\n",
        "\r\n",
        "def accuracy_vs_training_data(camembert_model, initial_weights,\r\n",
        "                              preprocessor, sizes,\r\n",
        "                              train_reviews, train_labels,\r\n",
        "                              val_reviews, val_labels,\r\n",
        "                              test_reviews, test_labels):\r\n",
        "    test_accuracies = []\r\n",
        "    X_val, y_val = preprocessor.transform(val_reviews, val_labels)\r\n",
        "    X_test, y_test = preprocessor.transform(test_reviews, test_labels)\r\n",
        "\r\n",
        "    for size in sizes:\r\n",
        "      # Preprocess data\r\n",
        "      X_train, y_train = preprocessor.fit_transform(\r\n",
        "          train_reviews[:size], train_labels[:size]\r\n",
        "      )\r\n",
        "\r\n",
        "      # Reset weights to initial value\r\n",
        "      camembert_model.set_weights(initial_weights)\r\n",
        "\r\n",
        "      best_model = EarlyStoppingModel(\r\n",
        "          camembert_model, max_epoches=20, batch_size=4,\r\n",
        "          validation_data=(X_val, y_val)\r\n",
        "      )\r\n",
        "\r\n",
        "      # Train model\r\n",
        "      best_model.fit(X_train, y_train)\r\n",
        "\r\n",
        "      # Evaluate on test set\r\n",
        "      y_pred = best_model.predict(X_test)\r\n",
        "      test_acc = metrics.accuracy_score(y_test, y_pred)\r\n",
        "      test_accuracies.append(test_acc)\r\n",
        "      print(\"Test acc: \" + str(test_acc))\r\n",
        "\r\n",
        "    return test_accuracies\r\n",
        "\r\n",
        "\r\n",
        "def encode_reviews(tokenizer, reviews, max_length):\r\n",
        "  token_ids = np.zeros(shape=(len(reviews), max_length), dtype=np.int32)\r\n",
        "  for i, review in enumerate(reviews):\r\n",
        "      encoded = tokenizer.encode(review, max_length=max_length)\r\n",
        "      token_ids[i, 0:len(encoded)] = encoded\r\n",
        "  attention_mask = (token_ids != 0).astype(np.int32)\r\n",
        "  return {\"input_ids\": token_ids, \"attention_mask\": attention_mask}\r\n",
        "\r\n",
        "\r\n",
        "def load_dataset(dataset_path):\r\n",
        "  with open(dataset_path, 'rb') as reader:\r\n",
        "      data = pickle.load(reader)\r\n",
        "  train_reviews = np.array(data[\"train_set\"]['review'])\r\n",
        "  val_reviews = np.array(data[\"val_set\"]['review'])\r\n",
        "  test_reviews = np.array(data[\"test_set\"]['review'])\r\n",
        "  train_labels = data[\"train_set\"]['polarity']\r\n",
        "  val_labels = data[\"val_set\"]['polarity']\r\n",
        "  test_labels = data[\"test_set\"]['polarity']\r\n",
        "  class_names = data['class_names']\r\n",
        "\r\n",
        "  return (train_reviews, train_labels, val_reviews, val_labels, test_reviews,\r\n",
        "          test_labels, class_names)\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KDubmKUikew3",
        "outputId": "875ec8f8-ea9e-40a2-bc6d-d5227953f01a"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\r\n",
        "device_name"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRLcxwgVZAsN"
      },
      "source": [
        "if device_name != '/device:GPU:0':\r\n",
        "  print(\r\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\r\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\r\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\r\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLLwJCTmOki8",
        "outputId": "814d3271-d84d-49cd-eee9-eb7bbc08e75c"
      },
      "source": [
        "tf.test.is_built_with_cuda()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gev2tf3XHDw"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u35YD4W27Qh",
        "outputId": "57f81ec8-fd65-44ba-e27b-00dfbeb30ff2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GGg6Cww5PTe"
      },
      "source": [
        "PICKLE_PATH = \"/content/drive/My Drive/allocine_dataset.pickle\"\r\n",
        "MAX_SEQ_LEN = 400 # in terms of generated tokens (not words)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzN-8m_yW7le"
      },
      "source": [
        "(train_reviews, train_labels, val_reviews, val_labels, test_reviews,\r\n",
        "  test_labels, class_names) = load_dataset(PICKLE_PATH)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS1G8BX7fzWm"
      },
      "source": [
        "# Load classification and pre-processing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyFza1CbW4J5",
        "outputId": "5e1352de-dc5a-45a7-b5a3-2236970331bc"
      },
      "source": [
        "def create_model():\r\n",
        "  model_name = \"camembert-base\"\r\n",
        "  tokenizer = CamembertTokenizer.from_pretrained(model_name)\r\n",
        "  preprocessor = CamembertPreprocessor(tokenizer, MAX_SEQ_LEN)\r\n",
        "\r\n",
        "  model = TFCamembertForSequenceClassification.from_pretrained('jplu/tf-camembert-base')\r\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=5e-6, epsilon=1e-08)\r\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "  model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
            "\n",
            "Some layers of TFCamembertForSequenceClassification were not initialized from the model checkpoint at jplu/tf-camembert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_camembert_for_sequence_classification_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "roberta (TFRobertaMainLayer) multiple                  110031360 \n",
            "_________________________________________________________________\n",
            "classifier (TFRobertaClassif multiple                  592130    \n",
            "=================================================================\n",
            "Total params: 110,623,490\n",
            "Trainable params: 110,623,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkso5otmvEn"
      },
      "source": [
        "tf.get_logger().setLevel('ERROR')\r\n",
        "model = create_model()\r\n",
        "initial_weights = model.get_weights()\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5jGDCa8f7un"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ChCv9e6f-xA"
      },
      "source": [
        "## Text encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZggrqanXoKf"
      },
      "source": [
        "#encoding input\r\n",
        "size = 10000\r\n",
        "X_train, y_train = preprocessor.fit_transform(\r\n",
        "        train_reviews[:size], train_labels[:size]\r\n",
        "    )\r\n",
        "X_val, y_val = preprocessor.transform(val_reviews[:size], val_labels[:size])\r\n",
        "X_test, y_test = preprocessor.transform(test_reviews[:size], test_labels[:size])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk_o6pvogAy3"
      },
      "source": [
        "## Training time with GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMGoCTpdXz5y",
        "outputId": "38422e1f-0d11-4df2-f711-d7ba844f3d28"
      },
      "source": [
        "epochs = 5\r\n",
        "batch_size = 16\r\n",
        "with tf.device('/device:GPU:0'):\r\n",
        "  tic = time.time()\r\n",
        "  model.fit(\r\n",
        "            X_train, y_train,\r\n",
        "            batch_size = batch_size,\r\n",
        "            validation_data=(X_val, y_val),\r\n",
        "            epochs=epochs,\r\n",
        "            #callbacks=[early_stopper],\r\n",
        "            #verbose=1\r\n",
        "        )\r\n",
        "  gpu_time = time.time() - tic \r\n",
        "  print('Total GPU time for {} texts on {} epochs (batch size {}) is {}'.format(\r\n",
        "      size, epochs, batch_size, str(round(gpu_time, 2))))  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "7/7 [==============================] - 25s 2s/step - loss: 0.6949 - accuracy: 0.4891 - val_loss: 0.6972 - val_accuracy: 0.4600\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 11s 2s/step - loss: 0.6916 - accuracy: 0.5753 - val_loss: 0.6969 - val_accuracy: 0.4600\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 11s 2s/step - loss: 0.6885 - accuracy: 0.5576 - val_loss: 0.6967 - val_accuracy: 0.4600\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 11s 2s/step - loss: 0.6838 - accuracy: 0.5758 - val_loss: 0.6960 - val_accuracy: 0.4600\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 11s 2s/step - loss: 0.6855 - accuracy: 0.5626 - val_loss: 0.6945 - val_accuracy: 0.4600\n",
            "Total GPU time for 100 texts on 5 epochs (batch size 16) is 68.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IntcuYeMaYB1",
        "outputId": "1406e180-25d5-4119-d53d-333ecc1a3c71"
      },
      "source": [
        "print('Total GPU time for {} texts on {} epochs (batch size {}) is {}'.format(size, epochs, batch_size, str(round(gpu_time, 2))))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total GPU time for 100 texts on 5 epochs (batch size 4) is 61.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDti6i6gfMYa"
      },
      "source": [
        "## Inference time with GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOYMGnNUd-Ub",
        "outputId": "7821b551-3eb9-4fb0-c3b2-898103665187"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\r\n",
        "  tic = time.time()\r\n",
        "  pred = model.predict(X_test)\r\n",
        "  gpu_time = time.time() - tic \r\n",
        "  print('Total GPU time for {} texts is {}'.format(size, str(round(gpu_time, 2))))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total GPU time for 1000 texts is 30.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-9coAQ4fHDX"
      },
      "source": [
        "## Training time with CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D15Nqn8RYfDL",
        "outputId": "dc53a3b1-c0d7-40f7-9e4f-a7245f3c900a"
      },
      "source": [
        "with tf.device('/cpu:0'):\r\n",
        "  tic = time.time()\r\n",
        "  model.fit(\r\n",
        "            X_train, y_train,\r\n",
        "            #use_multiprocessing=True,\r\n",
        "            #validation_data=self.validation_data,\r\n",
        "            epochs=2,\r\n",
        "            #batch_size=self.batch_size,\r\n",
        "            #callbacks=[early_stopper],\r\n",
        "            #verbose=1\r\n",
        "        )\r\n",
        "  gpu_time = time.time() - tic \r\n",
        "  print('Total CPU time for {} texts is {}'.format(size, str(round(gpu_time, 2))))\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEhkT67Wlqz5"
      },
      "source": [
        "## Training time with TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHHwDITsn59v"
      },
      "source": [
        "The code below is very slow on the first run=epoch (much slower than GPU, takes 77 sec. for 100 texts) but lightening fast on the next ones. \r\n",
        "Code adapted from [TPUs in Colab](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=hJl3vNtJOB-x). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx-YUrwPlqLP"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "print(\"Tensorflow version \" + tf.__version__)\r\n",
        "\r\n",
        "try:\r\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n",
        "except ValueError:\r\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0HiFz4jmaTh",
        "outputId": "68b693e8-450c-4874-cf8f-b2136cbb1d4f"
      },
      "source": [
        "with tpu_strategy.scope():\r\n",
        "  model = create_model()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
            "\n",
            "Some layers of TFCamembertForSequenceClassification were not initialized from the model checkpoint at jplu/tf-camembert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vVZzHdhlxf4",
        "outputId": "852380b9-f368-42e0-f6dd-f6635dcb228a"
      },
      "source": [
        "epochs = 5\r\n",
        "batch_size = 16\r\n",
        "tic = time.time()\r\n",
        "model.fit(\r\n",
        "          X_train, y_train,\r\n",
        "          batch_size = batch_size,\r\n",
        "          validation_data=(X_val, y_val),\r\n",
        "          epochs=epochs,\r\n",
        "          #callbacks=[early_stopper],\r\n",
        "          #verbose=1\r\n",
        "      )\r\n",
        "gpu_time = time.time() - tic \r\n",
        "print('Total GPU time for {} texts on {} epochs (batch size {}) is {}'.format(\r\n",
        "    size, epochs, batch_size, str(round(gpu_time, 2))))  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 128s 128ms/step - loss: 0.1613 - accuracy: 0.9488 - val_loss: 0.1202 - val_accuracy: 0.9611\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 74s 119ms/step - loss: 0.1113 - accuracy: 0.9652 - val_loss: 0.1149 - val_accuracy: 0.9608\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 75s 121ms/step - loss: 0.0824 - accuracy: 0.9760 - val_loss: 0.1234 - val_accuracy: 0.9601\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 75s 120ms/step - loss: 0.0614 - accuracy: 0.9822 - val_loss: 0.1290 - val_accuracy: 0.9589\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 75s 120ms/step - loss: 0.0462 - accuracy: 0.9879 - val_loss: 0.1435 - val_accuracy: 0.9606\n",
            "Total GPU time for 10000 texts on 5 epochs (batch size 16) is 428.62\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}